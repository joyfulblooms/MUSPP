{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joyfulblooms/MUSPP/blob/main/Testing_HDB_Resale_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAND5BfhTj_M"
      },
      "source": [
        "# HDB Resale Price Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCWmdQzTTj_P"
      },
      "outputs": [],
      "source": [
        "# import pandas and numpy library\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxd_JI8vTj_P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import seaborn as sns\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwA6RVRvTj_Q"
      },
      "source": [
        "# 1 Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt1P98kvTj_Q",
        "outputId": "ec38b159-1bd9-4f43-d1d9-d9c5eea70ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3bce50d42aed>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# check data file; updated the 2017 onwards file to be the latest dataset as of 17 Apr 2023. Decided that the 2017 onwards dataset is sufficient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/'"
          ]
        }
      ],
      "source": [
        "# check data file; updated the 2017 onwards file to be the latest dataset as of 17 Apr 2023. Decided that the 2017 onwards dataset is sufficient. \n",
        "os.listdir(\"data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgO_-4klTj_R"
      },
      "outputs": [],
      "source": [
        "# read data\n",
        "\n",
        "data_2017 = pd.read_csv(\"data/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab6U-Um8Tj_R"
      },
      "outputs": [],
      "source": [
        "# check the first several lines of data_2017 with head method of pandas dataframe\n",
        "data_2017.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITmwlctITj_R"
      },
      "outputs": [],
      "source": [
        "# convert the month column to datetime type\n",
        "data_2017['month'] = pd.to_datetime(data_2017['month'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xETVKfYXTj_S"
      },
      "outputs": [],
      "source": [
        "# check the info of data_wide\n",
        "data_2017.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ue9zQyTTj_S"
      },
      "source": [
        "month : month and year of transaction <br/>\n",
        "town : town of flat <br/>\n",
        "flat_type : type of flat <br/>\n",
        "block : block number of flat <br/>\n",
        "street_name : street name of flat <br/>\n",
        "storey_range : storey of flat <br/>\n",
        "floor_area_sqm : floor area of flat in squared meter <br/>\n",
        "flat_model : model of flat <br/>\n",
        "lease_commence_date : date the lease started <br/>\n",
        "resale_price : nominal resale price of flat <br/>\n",
        "remaining_lease : remaining lease of flat in years <br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr1kEz8pTj_S"
      },
      "source": [
        "# 2 Data Analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oHrMZDMTj_S"
      },
      "source": [
        "## 2.1 Feature viewing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AskzyA4mTj_T"
      },
      "source": [
        "Because we have a limited number of features and they all have specific meanings, we can look at the values of each feature in turn and perform basic statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K85SGzw5Tj_T"
      },
      "source": [
        "### 2.1.1 month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "AcPU59oeTj_T"
      },
      "outputs": [],
      "source": [
        "data_2017.groupby(pd.Grouper(key='month', freq='M')).apply(lambda x: len(x)).plot.area()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o1WoTaDTj_T"
      },
      "source": [
        "In terms of time distribution, the sample is relatively uniform except that we see a significant dip in 2020 due to COVID. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7r-dpjMTj_T"
      },
      "source": [
        "### 2.1.2 town"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0tal0fWvTj_T"
      },
      "outputs": [],
      "source": [
        "# count the number of every town in the 'town' column of data_wide using value_counts method\n",
        "data_2017['town'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwms8j_ITj_T"
      },
      "outputs": [],
      "source": [
        "# Function for lollipop charts\n",
        "def loll_plot(df, x, y, xlabel, xlim):\n",
        "    plt.rc('axes', axisbelow=True)\n",
        "    plt.grid(linestyle='--', alpha=0.4)\n",
        "    # plt.hlines\n",
        "    plt.hlines(y=df.index, xmin=0, xmax=df[x], color=df.color, linewidth=1)\n",
        "    plt.scatter(df[x], df.index, color=df.color, s=300)\n",
        "    for i, txt in enumerate(df[x]):\n",
        "        plt.annotate(str(round(txt)), (txt, i), color='white', fontsize=9, ha='center', va='center')\n",
        "    plt.yticks(df.index, df[y])\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.xlim(xlim)\n",
        "    plt.xlabel(xlabel, fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrmF5M7_Tj_U"
      },
      "outputs": [],
      "source": [
        "# proprocessing data for plotting\n",
        "data_wide_town = data_2017.copy()\n",
        "data_wide_town['year'] = pd.DatetimeIndex(data_wide_town['month']).year # extract out year\n",
        "data_wide_town = data_wide_town.groupby(['town'], as_index=False).agg(\n",
        "    {'resale_price': 'median'}).sort_values('resale_price', ascending=True).reset_index(drop=True)\n",
        "data_wide_town['resale_price'] = round(data_wide_town['resale_price']/1000)\n",
        "data_wide_town['color'] = ['#f8766d'] + ['#3c78d8']*(len(data_wide_town)-2) + ['#00ba38']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtNMfnjMTj_U"
      },
      "outputs": [],
      "source": [
        "#4-room flats\n",
        "price_4room = data_2017[data_2017['flat_type'] == '4 ROOM'].copy()\n",
        "price_4room['year'] = pd.DatetimeIndex(price_4room['month']).year # extract out year\n",
        "price_4room = price_4room.groupby(['town'], as_index=False).agg(\n",
        "    {'resale_price': 'median'}).sort_values('resale_price', ascending=True).reset_index(drop=True)\n",
        "price_4room['resale_price'] = round(price_4room['resale_price']/1000)\n",
        "price_4room['color'] = ['#f8766d'] + ['#3c78d8']*(len(price_4room)-2) + ['#00ba38']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F1rl5o7Tj_U"
      },
      "outputs": [],
      "source": [
        "# create subplots\n",
        "fig = plt.subplots(1, 2, figsize=(14, 7))\n",
        "\n",
        "# plot lollipop graph for whole town's resale price\n",
        "ax1 = plt.subplot(121)\n",
        "loll_plot(data_wide_town, 'resale_price', 'town', 'Resale Price (SGD)', [100, 1000])\n",
        "xlabels = ax1.get_xticks().tolist()\n",
        "ax1.xaxis.set_major_locator(mticker.FixedLocator(xlabels))\n",
        "ax1.set_xticklabels(['%.0f K'% x for x in xlabels])\n",
        "ax1.yaxis.set_ticks_position('none')\n",
        "ax1.set_title('Median Resale Price of All Flats')\n",
        "\n",
        "# plot lollipop graph for 4 Room flats resale price\n",
        "ax2 = plt.subplot(122)\n",
        "loll_plot(price_4room, 'resale_price', 'town', 'Resale Price (SGD)', [100, 1000])\n",
        "xlabels = ax2.get_xticks().tolist()\n",
        "ax2.xaxis.set_major_locator(mticker.FixedLocator(xlabels))\n",
        "ax2.set_xticklabels(['%.0f K'% x for x in xlabels])\n",
        "ax2.yaxis.set_ticks_position('none')\n",
        "ax2.set_title('Median Resale Price of 4 Room Flats')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TvePcydTj_U"
      },
      "source": [
        "We need to see both the Median Resale Prices of all the flats and the Median Resale Prices of 4 Room flats. This is because the former graph paints a slightly inaccurate picture. It shows ang mo kio as having the lowest resale price but it is mainly because the flats sold there are mostly 3room and 2room flats. The graph of the median prices of 4 room flats paint a more accurate picture and matches our intuition (central areas cost more, northern areas and non-mature estates cost less). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzq4vPXnTj_U"
      },
      "source": [
        "### 2.1.3 flat_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RinpRPhbTj_U"
      },
      "outputs": [],
      "source": [
        "data_2017['flat_type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaWatRo0Tj_U"
      },
      "outputs": [],
      "source": [
        "data_2017['flat_type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajjUzEXiTj_V"
      },
      "outputs": [],
      "source": [
        "# calculate ratio for every flat_type\n",
        "flat_type_ratio = [val/ data_2017.shape[0] for val in data_2017['flat_type'].value_counts().values]\n",
        "flat_type_label = data_2017['flat_type'].value_counts().keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlyoPT39Tj_V"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.pie(\n",
        "    flat_type_ratio,\n",
        "    explode=[0.005, 0.005, 0.005, 0.005, 0.05, 0.2, 0.5],\n",
        "    labels=flat_type_label,\n",
        "    autopct='%1.1f%%',\n",
        "    shadow=False,\n",
        "    startangle=90,\n",
        "    pctdistance=1\n",
        ")\n",
        "ax.axis('equal')\n",
        "plt.title(\"Pie chart of flat type\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cb0Yp-jTj_V"
      },
      "source": [
        "In terms of flat types, 3-5ROOM is the most common. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZRTwRcJTj_V"
      },
      "outputs": [],
      "source": [
        "#We will drop 1 Room Flat and Multi-Generation Flats since the counts are small and they are not the focus of our study. \n",
        "data_2017 = data_2017[(data_2017['flat_type'] != 'MULTI-GENERATION') & (data_2017['flat_type'] != '1 ROOM')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAEUo5SbTj_V"
      },
      "source": [
        "### 2.1.4 block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ_mI1TUTj_V"
      },
      "outputs": [],
      "source": [
        "#data_wide['block'].value_counts()\n",
        "data_2017['block'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXBkWa8nTj_V"
      },
      "source": [
        "### 2.1.5 street_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRpcjizmTj_V"
      },
      "outputs": [],
      "source": [
        "#data_wide['street_name'].value_counts()\n",
        "data_2017['street_name'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi8kwL5oTj_V"
      },
      "source": [
        "### 2.1.6 storey_range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgmHGfduTj_V"
      },
      "outputs": [],
      "source": [
        "#data_wide['storey_range'].value_counts()\n",
        "data_2017['storey_range'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "7pA05-tITj_V"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12,4))\n",
        "ax1 = plt.subplot(111)\n",
        "#storey = data_wide.groupby('storey_range')['resale_price'].median().reset_index().sort_values(by='storey_range')\n",
        "storey = data_2017.groupby('storey_range')['resale_price'].median().reset_index().sort_values(by='storey_range')\n",
        "storey['storey_rank'] = storey['storey_range'].astype('category').cat.codes # label encode\n",
        "sns.scatterplot(x=storey['storey_rank'], \n",
        "                  y=storey['resale_price'], \n",
        "                  size=storey['storey_rank'].astype('int')*30, \n",
        "                  color='green',\n",
        "                  edgecolors='w', \n",
        "                  alpha=0.5, legend=False, ax=ax1)\n",
        "ylabels = ax1.get_yticks().tolist()\n",
        "ax1.yaxis.set_major_locator(mticker.FixedLocator(ylabels))\n",
        "ax1.set_yticklabels(['%.0f K'% (x/1000) for x in ylabels])\n",
        "xlabels = ax1.get_xticks().tolist()\n",
        "print(xlabels)\n",
        "ax1.xaxis.set_major_locator(mticker.FixedLocator(xlabels))\n",
        "ax1.set_xticklabels([''] + list(storey['storey_range'].iloc[[0, 5, 10, 15, 20, 24]]) + [''])\n",
        "ax1.set_ylim([280000,1200000]), ax1.set_ylabel('Resale Price SGD ($)', size=15), ax1.set_xlabel('Storey', size=15)\n",
        "ax1.set_title('The impact of storey range on resale price', size=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbpV1aC_Tj_W"
      },
      "source": [
        "The higher the storey, the higher the general housing price. This matches our intuition. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrzyTjUsTj_W"
      },
      "source": [
        "### 2.1.7 floor_area_sqm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XGxNfUWTj_W"
      },
      "outputs": [],
      "source": [
        "#data_wide['floor_area_sqm'].value_counts()\n",
        "data_2017['floor_area_sqm'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6tVo60gTj_W"
      },
      "outputs": [],
      "source": [
        "# create a histogram plot of the floor_area_sqm column with bins=50 and edgecolor='black' using plt.hist\n",
        "plt.hist(data_2017['floor_area_sqm'], bins=50, edgecolor ='black')\n",
        "plt.title('Distribution of HDB Floor Area')\n",
        "plt.show()\n",
        "display(data_2017[data_2017['floor_area_sqm'] > 200]['flat_model'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHN09zjaTj_W"
      },
      "source": [
        "The housing area is concentrated around 100. This matches our intuition as most flats sold are 4Room or 5Room flats. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYmmZUidTj_W"
      },
      "source": [
        "### 2.1.8 flat_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63mqJaxxTj_W"
      },
      "outputs": [],
      "source": [
        "#data_wide['flat_model'].value_counts()\n",
        "data_2017['flat_model'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bsnnEwgTj_W"
      },
      "outputs": [],
      "source": [
        "#There are many flat_models and it is best to reduce it through recategorisation\n",
        "replace_values = {'Premium Maisonette':'Maisonette', 'Model A-Maisonette':'Maisonette','Improved-Maisonette':'Maisonette','Terrace':'Special', 'Adjoined flat':'Special', \n",
        "                'DBSS':'Special', 'Premium Apartment Loft':'Special','3Gen':'Special', 'Model A2':'Model A', 'Premium Apartment':'Apartment', 'Improved':'Standard', 'Simplified':'Model A', '2-room':'Standard Small', 'Type S1':'Standard Small', 'Type S2':'Standard Small'}\n",
        "data_2017 = data_2017.replace({'flat_model': replace_values})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgZlbEzETj_W"
      },
      "outputs": [],
      "source": [
        "data_2017['flat_model'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL6JNRzlTj_W"
      },
      "source": [
        "We have reduced the number of categories of flats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHslYWuYTj_W"
      },
      "source": [
        "### 2.1.9 lease_commence_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "c3pp26plTj_W"
      },
      "outputs": [],
      "source": [
        "#data_wide['lease_commence_date'].value_counts()\n",
        "data_2017['lease_commence_date'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TExJICSZTj_W"
      },
      "outputs": [],
      "source": [
        "#bins = data_wide['lease_commence_date'].max() - data_wide['lease_commence_date'].min()\n",
        "#plt.hist(data_wide['lease_commence_date'], bins=bins, edgecolor='black')\n",
        "bins = data_2017['lease_commence_date'].max() - data_2017['lease_commence_date'].min()\n",
        "plt.hist(data_2017['lease_commence_date'], bins=bins, edgecolor='black')\n",
        "plt.title('Distribution of Lease Commence Year')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q1sKqGtTj_X"
      },
      "source": [
        "### 2.1.10 resale_price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swUhoPsFTj_X"
      },
      "outputs": [],
      "source": [
        "#data_wide['resale_price']\n",
        "data_2017['resale_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ2nEVgdTj_X"
      },
      "outputs": [],
      "source": [
        "#price_range = pd.cut(data_wide['resale_price'], bins=10).value_counts().items()\n",
        "price_range = pd.cut(data_2017['resale_price'], bins=10).value_counts().items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTSODf_mTj_X"
      },
      "outputs": [],
      "source": [
        "#price_range = pd.cut(data_wide['resale_price'], bins=10).value_counts().items()\n",
        "price_range = pd.cut(data_2017['resale_price'], bins=10).value_counts().items()\n",
        "nums = []\n",
        "xlabels = []\n",
        "for key, val in price_range:\n",
        "    xlabels.append(key)\n",
        "    nums.append(val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezz17cnJTj_X"
      },
      "source": [
        "Partition house prices equidistant to see the number of samples for different price ranges. The intuitive conclusion is that the lower the price, the more samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chHTYhhsTj_X"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.bar(np.arange(1, len(nums)+1), nums)\n",
        "plt.xticks(np.arange(1, len(nums)+1), xlabels, rotation=-90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "glqNgnlfTj_X"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "#ax.hist(data_wide['resale_price'], 20)\n",
        "ax.hist(data_2017['resale_price'], 20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrDuqxlHTj_X"
      },
      "source": [
        "The histogram effect without partitioning is also consistent. The sample size is mainly concentrated in the low price segment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMoziBfNTj_X"
      },
      "source": [
        "### 2.1.11 remaining_lease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_kDrd-XTj_X"
      },
      "outputs": [],
      "source": [
        "#data_wide['remaining_lease'].value_counts()\n",
        "data_2017['remaining_lease'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsHv7X37Tj_X"
      },
      "source": [
        "## 2.2 Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVQ6y6OqTj_X"
      },
      "source": [
        "### 2.2.1 value substitution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q9FjvYJTj_X"
      },
      "source": [
        "Making all categorical fields lowercase helps combine values that have the same meaning but different letter cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJdF8vETTj_X"
      },
      "outputs": [],
      "source": [
        "#data_wide_c = data_wide.copy() # make a copy of data_wide for further processing\n",
        "data_wide_c = data_2017.copy() # make a copy of data_wide for further processing\n",
        "for var in ['town', 'flat_type', 'block', 'street_name', 'storey_range', 'flat_model']:\n",
        "    data_wide_c[var] = data_wide_c[var].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbIT2tyHTj_Y"
      },
      "outputs": [],
      "source": [
        "data_wide_c['flat_type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARWXfpuQTj_Y"
      },
      "outputs": [],
      "source": [
        "data_wide_c['flat_model'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqjfGilLTj_Y"
      },
      "source": [
        "### 2.2.2 Inflation Adjustment Using CPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXtzG77VTj_Y"
      },
      "source": [
        "Prices in different years will be affected by inflation, and 2019 will be used as a baseline to correct the impact of inflation to more accurately compare price changes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydVSySHVTj_Y"
      },
      "outputs": [],
      "source": [
        "# Compute Resale Price Adjusted for Inflation Using Consumer Price Index for Housing & Utilities\n",
        "# https://github.com/teyang-lau/HDB_Resale_Prices/blob/main/Data/CPI.csv \n",
        "# click raw -> right click -> save as\n",
        "#I have included Mar 2023 and Apr 2023 CPI as the same as Feb 2023 as the runrate is similar and the monetary policies have not changed in those months so it is reasonable to assume that the rate is similar to Feb 2023. \n",
        "cpi = pd.read_csv(\"data/CPI_2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIfP92S9Tj_Y"
      },
      "outputs": [],
      "source": [
        "cpi.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vazGPWpmTj_Y"
      },
      "outputs": [],
      "source": [
        "cpi.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkB-YstATj_Y"
      },
      "outputs": [],
      "source": [
        "cpi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liROgZOZTj_Y"
      },
      "outputs": [],
      "source": [
        "cpi['month'] = pd.to_datetime(cpi['month'], format='%Y %b', errors='coerce') # to datetime\n",
        "data_wide_c['month'] = pd.to_datetime(data_wide_c['month'], format='%Y-%m')\n",
        "# merge data_wide_c and cpi on the column month and how='left' into a new data_wide_c\n",
        "data_wide_c = data_wide_c.merge(cpi, on='month', how='left')\n",
        "# https://people.duke.edu/~rnau/411infla.htm\n",
        "data_wide_c['real_price'] = (data_wide_c['resale_price'] / data_wide_c['cpi']) * 100 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcjNg68RTj_Y"
      },
      "outputs": [],
      "source": [
        "data_wide_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAQjxPXwTj_Y"
      },
      "outputs": [],
      "source": [
        "# plot the unadjusted and adjusted median price for each year\n",
        "# Unadjusted\n",
        "fig = plt.figure(figsize=(14,4.5))\n",
        "fig.suptitle('Median HDB Resale Prices Over the Years', fontsize=18)\n",
        "ax1 = fig.add_subplot(121)\n",
        "data_wide_c.groupby('month')[['resale_price']].median().plot(ax=ax1, color='green', legend=None)\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Resale Price in SGD ($)')\n",
        "ax1.set_ylim(0, 700000)\n",
        "ax1.set_title('Unadjusted for Inflation', size=15)\n",
        "\n",
        "# Adjusted\n",
        "ax2 = fig.add_subplot(122)\n",
        "data_wide_c.groupby('month')[['real_price']].median().plot(ax=ax2, color='blue', legend=None)\n",
        "ax2.set_xlabel('Date')\n",
        "ax2.set_ylabel('Resale Price in SGD ($)')\n",
        "ax2.set_ylim(0, 700000)\n",
        "ax2.set_title('Adjusted for Inflation',size=15)\n",
        "plt.tight_layout() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVmMvVZJTj_Y"
      },
      "source": [
        "### 2.2.3 Convert remaining_lease to number of years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kdlo6fLeTj_Y"
      },
      "outputs": [],
      "source": [
        "data_wide_c['remaining_lease'].unique()[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-GIPTkjTj_Y"
      },
      "outputs": [],
      "source": [
        "# write a function to normalize lease year\n",
        "def getYears(text):\n",
        "    if isinstance(text, str):\n",
        "        yearmonth = [int(s) for s in text.split() if s.isdigit()]\n",
        "        if len(yearmonth) > 1: # if there's year and month\n",
        "            years = yearmonth[0] + (yearmonth[1]/12)\n",
        "        else: # if only year\n",
        "            years = yearmonth[0]\n",
        "        return years\n",
        "    else: # if int\n",
        "        return text\n",
        "\n",
        "data_wide_c['remaining_lease_year'] = data_wide_c['remaining_lease'].apply(lambda x: getYears(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sikX4qULTj_Z"
      },
      "outputs": [],
      "source": [
        "data_wide_c['remaining_lease_year'].hist(bins=50)\n",
        "plt.title('Distribution of Remaining Lease for 2017-2023 Data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEwRybYJTj_Z"
      },
      "source": [
        "The output makes sense as typically, many owners sell their homes at the 5 years mark. There are also interesting peaks at the 35 year mark and the 20 year mark. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1sLTxWcTj_Z"
      },
      "source": [
        "### 2.2.4 missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKsvEYDPTj_Z"
      },
      "outputs": [],
      "source": [
        "data_wide_c.isnull().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5bYCb-JTj_Z"
      },
      "source": [
        "There are no missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhA2R8SATj_Z"
      },
      "outputs": [],
      "source": [
        "#Change lease commence date to datetime\n",
        "data_wide_c['lease_commence_date'] = pd.to_datetime(data_wide_c['lease_commence_date'], format='%Y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uajV-SQLTj_Z"
      },
      "source": [
        "## 2.3 EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlM9Z2EnTj_Z"
      },
      "source": [
        "### 2.3.1 time/date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_y93ZvwTj_Z"
      },
      "source": [
        "Features are derived by transaction date, start lease date, and remaining lease duration. <br/>\n",
        "Includes the number of years since the initial lease date; Total lease term, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwspMRc7Tj_Z"
      },
      "outputs": [],
      "source": [
        "# The number of years from the date of the transaction to the start of the lease.\n",
        "data_wide_c['year_of_transaction_from_lease'] = (pd.to_datetime(data_wide_c['month'].map(str))-\n",
        "                                                 pd.to_datetime(data_wide_c['lease_commence_date'].map(str))).dt.days/365"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfGHwb0bTj_Z"
      },
      "outputs": [],
      "source": [
        "data_wide_c['year_of_transaction_from_lease']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZyHhXFJTj_Z"
      },
      "outputs": [],
      "source": [
        "data_wide_c['year_of_transaction_from_lease'].hist(bins=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LASPVqdvTj_Z"
      },
      "outputs": [],
      "source": [
        "# Lease years plus remaining lease years = total lease years.\n",
        "data_wide_c['whole_years_from_lease'] = data_wide_c['year_of_transaction_from_lease'] + data_wide_c['remaining_lease_year']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsVK8B22Tj_Z"
      },
      "outputs": [],
      "source": [
        "data_wide_c['whole_years_from_lease'].hist(bins=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kda7KeUTj_a"
      },
      "source": [
        "This meets our expectations as the typical lease for HDB flats is 99 years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSItCxQITj_a"
      },
      "source": [
        "### 2.3.2 coordinates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs3HRR5PTj_a"
      },
      "source": [
        "The latitude and longitude coordinates corresponding to each block can be obtained through the open source interface. After obtaining the latitude and longitude coordinates, the location can be intuitively marked out and the area in the sample set can be observed. And can use longitude and latitude coordinates for clustering and other operations, through the distance to judge the house price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAMl_P73Tj_a"
      },
      "outputs": [],
      "source": [
        "## Function for getting postal code, geo coordinates of addresses, and write to a csv file\n",
        "def find_postal(lst, filename):\n",
        "    '''With the block number and street name, get the full address of the hdb flat,\n",
        "    including the postal code, geogaphical coordinates (lat/long)'''\n",
        "    \n",
        "    for index,add in enumerate(lst):\n",
        "        # Do not need to change the URL\n",
        "        url= \"https://developers.onemap.sg/commonapi/search?returnGeom=Y&getAddrDetails=Y&pageNum=1&searchVal=\"+ add\n",
        "        if index % 100 == 0:\n",
        "            print(index,url)\n",
        "        \n",
        "        # Retrieve information from website\n",
        "        response = requests.get(url)\n",
        "        try:\n",
        "            data = json.loads(response.text) \n",
        "        except ValueError:\n",
        "            print('JSONDecodeError')\n",
        "            pass\n",
        "    \n",
        "        temp_df = pd.DataFrame.from_dict(data[\"results\"])\n",
        "        # The \"add\" is the address that was used to search in the website\n",
        "        temp_df[\"address\"] = add\n",
        "        \n",
        "        # Create the file with the first row that is read in \n",
        "        if index == 0:\n",
        "            file = temp_df\n",
        "        else:\n",
        "            file = file.append(temp_df)\n",
        "    file.to_csv(filename + '.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "iZczuOi7Tj_a"
      },
      "outputs": [],
      "source": [
        "wide_block_street = data_wide_c[['block', 'street_name']]\n",
        "address= (wide_block_street['block'].map(lambda x: str(x).upper()) + ' ' \n",
        "                                + wide_block_street['street_name'].map(lambda x: str(x).upper()))\n",
        "wide_block_street.loc[:, 'address'] = address\n",
        "all_address = list(wide_block_street['address'])\n",
        "unique_address = list(set(all_address))\n",
        "\n",
        "print('Unique addresses:', len(unique_address))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MtlOwpITj_a"
      },
      "outputs": [],
      "source": [
        "# check the first five address in the list unique_address\n",
        "for i in range(5):\n",
        "    print (unique_address[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4q7qHG01Tj_a"
      },
      "outputs": [],
      "source": [
        "# this would run for nearly an hour\n",
        "# so when the flat_coordinates.csv file have been created once, you can comment this code out in order not to run it again.\n",
        "#find_postal(unique_address, 'data/flat_coordinates_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TxZgx2V-Tj_a"
      },
      "outputs": [],
      "source": [
        "flat_coordinates = pd.read_csv(\"data/flat_coordinates_2.csv\")\n",
        "flat_coordinates.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlmIugkETj_a"
      },
      "outputs": [],
      "source": [
        "flat_coordinates = flat_coordinates[['address', 'LATITUDE', 'LONGITUDE']]\n",
        "flat_coordinates.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhEmOb-YTj_a"
      },
      "outputs": [],
      "source": [
        "data_wide_c['address'] = wide_block_street['address']\n",
        "data_wide_c2 = pd.merge(data_wide_c, flat_coordinates, on=['address'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0mUISlATj_a"
      },
      "outputs": [],
      "source": [
        "data_wide_c2['LATITUDE'].isnull().mean(), data_wide_c2['LONGITUDE'].isnull().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kWyMZvgTj_a"
      },
      "source": [
        "There are no null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "V20wBWdcTj_b"
      },
      "outputs": [],
      "source": [
        "data_wide_c2['street_name'].value_counts().describe([i/20 for i in range(20)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac7xg_jeTj_b"
      },
      "outputs": [],
      "source": [
        "street_name_static = pd.DataFrame(data_wide_c2['street_name'].value_counts().items(), columns=['street_name', 'num'])\n",
        "street_name_most = list(street_name_static[street_name_static['num']>1500]['street_name']) #I changed the listing of the number to be more than 1500 instead 10,000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "C4khiG05Tj_b"
      },
      "outputs": [],
      "source": [
        "print(street_name_most)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "HxCZQ1LGTj_b"
      },
      "outputs": [],
      "source": [
        "# plot location\n",
        "data_wide_c2_street_most = data_wide_c2[data_wide_c2['street_name'].isin(street_name_most)] #create dataframe to filter just the rows where the street_name column is in the list of street_name_most \n",
        "data_wide_c2_street_most = data_wide_c2_street_most.dropna(subset=['LONGITUDE', 'LATITUDE', 'street_name'], how='any') #drop values which are missing in either the columns\n",
        "color_dict = dict(zip(data_wide_c2_street_most['street_name'], data_wide_c2_street_most['street_name'].astype('category').cat.codes)) #dict where street names and values are integer codes tt rep colors for each street.\n",
        "color_dict = sorted(color_dict.items(), key=lambda x: x[1]) #sort dict by values in ascending order and return a list of tuples. \n",
        "print(color_dict)  # Correspondence between street names and color numbers\n",
        "scatter=plt.scatter(data_wide_c2_street_most['LONGITUDE'], data_wide_c2_street_most['LATITUDE'], s=1, \n",
        "                    c=data_wide_c2_street_most['street_name'].astype('category').cat.codes)\n",
        "plt.legend(handles=scatter.legend_elements()[0],\n",
        "          labels=[x[0] for x in color_dict])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqrAJAxMTj_b"
      },
      "source": [
        "### 2.3.3 one-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNUZv-enTj_b"
      },
      "source": [
        "All categorical variables are one-hot encoded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anAR8IOpTj_b"
      },
      "outputs": [],
      "source": [
        "data_wide_c3 = data_wide_c2.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr10h2dLTj_b"
      },
      "outputs": [],
      "source": [
        "data_wide_c3.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Tgdi-9yTj_b"
      },
      "outputs": [],
      "source": [
        "#Create a new column called region to reduce the dimensionality of the areas.\n",
        "d_region={'ang mo kio':'North East Mature','bedok':'East Mature','bishan':'Central Mature','bukit batok':'West Non-Mature','bukit merah':'Central Mature','bukit panjang':'West Non-Mature','bukit timah':'Central Mature','central area':'Central Mature','choa chu kang':'West Non-Mature','clementi':'West Mature','geylang':'Central Mature','hougang':'North East Non-Mature','jurong east':'West Non-Mature','jurong west':'West Non-Mature','kallang/whampoa':'Central Mature','marine parade':'Central Mature','pasir ris':'East Mature','punggol':'North East Non-Mature','queenstown':'Central Mature','sembawang':'North Non-Mature','sengkang':'North East Non-Mature','serangoon':'North East Mature','tampines':'East Mature','toa payoh':'Central Mature','woodlands':'North Non-Mature','yishun':'North Non-Mature'}\n",
        "data_wide_c3['region'] = data_wide_c3['town'].map(d_region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE_v4QuETj_b"
      },
      "outputs": [],
      "source": [
        "data_wide_c3['region'].isnull().mean() #Check for null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4yyGtzZTj_b"
      },
      "outputs": [],
      "source": [
        "# Check if the 'region' column contains the value 'Central Mature'\n",
        "if 'Central Mature' in data_wide_c3['region'].values:\n",
        "    print(\"The 'region' column contains 'Central Mature' value.\")\n",
        "else:\n",
        "    print(\"The 'region' column does not contain 'Central Mature' value.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-fp1z2rTj_b"
      },
      "outputs": [],
      "source": [
        "dummy_res = []\n",
        "for var in ['region', 'flat_type', 'storey_range', 'flat_model']:\n",
        "    dummy_data = pd.get_dummies(data_wide_c3[var], prefix=var, dummy_na = False, drop_first = False)\n",
        "    dummy_res.append(dummy_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4B_9-7DTj_c"
      },
      "outputs": [],
      "source": [
        "dummy_res[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTeUfV-WTj_c"
      },
      "outputs": [],
      "source": [
        "dummy_wide = pd.concat(dummy_res, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcJU514STj_c"
      },
      "outputs": [],
      "source": [
        "data_wide_c3 = pd.concat([data_wide_c3, dummy_wide], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I_5FBVlTj_c"
      },
      "outputs": [],
      "source": [
        "if \"region_Central Mature\" in data_wide_c3.columns:\n",
        "    print(\"The column 'region Central Mature' is present in the DataFrame.\")\n",
        "else:\n",
        "    print(\"The column 'region Central Mature' is not present in the DataFrame.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg5Yid9sTj_c"
      },
      "outputs": [],
      "source": [
        "data_wide_c3.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jZUHFL_Tj_c"
      },
      "outputs": [],
      "source": [
        "data_wide_c3['region'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzVf8VcwTj_c"
      },
      "outputs": [],
      "source": [
        "data_wide_c3.groupby('region')['real_price'].median()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmCyXsNYTj_c"
      },
      "source": [
        "We can see that HDB in the central region cost the most followed by the East region. Strangely the North East Non-Mature estates also look like they are priced quite highly. This could be due to a large number of flats sold in the North East Non-Mature region - Yishun - where the lease left is high that impacts the price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49rw_z42Tj_c"
      },
      "source": [
        "### 2.3.4 label encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iDIgI3KTj_c"
      },
      "outputs": [],
      "source": [
        "storey_dicts = sorted(list(data_wide_c3['storey_range'].unique()))\n",
        "{storey_dicts[i]:i for i in range(len(storey_dicts))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsIPukGFTj_c"
      },
      "outputs": [],
      "source": [
        "data_wide_c3['storey_range_label'] = data_wide_c3['storey_range'].replace({storey_dicts[i]:i for i in range(len(storey_dicts))}).astype('category').cat.codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DefWP5emTj_c"
      },
      "source": [
        "Include dataset on amenities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f-vTtndTj_c"
      },
      "outputs": [],
      "source": [
        "#Distance to the nearest amenities\n",
        "flat_amenities = pd.read_csv('data/flat_amenities.csv')\n",
        "\n",
        "# Merge amenities data to flat data. Realised that we need all the names of the block and street name to be in str and upper case or it will not be merged correctly. \n",
        "data_wide_c3['flat'] = data_wide_c3['block'].str.upper() + ' ' + data_wide_c3['street_name'].str.upper()\n",
        "data_wide_c3 = data_wide_c3.merge(flat_amenities, on='flat', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80OGlA6NTj_d"
      },
      "outputs": [],
      "source": [
        "#Check if they are correctly merged. \n",
        "null_counts = data_wide_c3.isnull().sum()\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVdaYA12Tj_d"
      },
      "outputs": [],
      "source": [
        "# get median info of each town\n",
        "tmp = data_wide_c3.groupby('town')[['park_dist','num_park_2km','mall_dist','num_mall_2km','mrt_dist','num_mrt_2km','real_price']].median().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3BNFHFOTj_d"
      },
      "outputs": [],
      "source": [
        "# scatterplot for median price of each town against nearest distance from each amenity\n",
        "\n",
        "p=sns.pairplot(tmp, x_vars=[\"park_dist\", \"mall_dist\", \"mrt_dist\"], y_vars=[\"real_price\"], height=3, aspect=1, kind=\"reg\", plot_kws=dict(scatter_kws=dict(s=40)))\n",
        "axes=p.axes\n",
        "ylabels = ['{:,.0f}'.format(x) + 'K' for x in axes[0,0].get_yticks()/1000]\n",
        "axes[0,0].set_yticklabels(ylabels), axes[0,0].set_ylabel('Resale Price SGD ($)', size=10)\n",
        "axes[0,0].set_xlabel('Distance From Park (km)', size=10), axes[0,1].set_xlabel('Distance From Mall (km)', size=10),axes[0,2].set_xlabel('Distance From MRT (km)', size=10)\n",
        "plt.suptitle('Resale Price (Median of Each Town) VS Distance from Nearest Amenities (Median of Each Town)')\n",
        "plt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygOPLu8oTj_d"
      },
      "source": [
        "We can see that distance from park, mall and MRT have a negative correlation with the price of the HDB. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbMMpR6cTj_d"
      },
      "outputs": [],
      "source": [
        "# scatterplot for median price of each town against number of amenities\n",
        "\n",
        "p=sns.pairplot(tmp, x_vars=[\"num_park_2km\", \"num_mall_2km\", \"num_mrt_2km\"], y_vars=[\"real_price\"], height=3, aspect=1, kind=\"reg\", plot_kws=dict(scatter_kws=dict(s=40)))\n",
        "axes=p.axes\n",
        "ylabels = ['{:,.0f}'.format(x) + 'K' for x in axes[0,0].get_yticks()/1000]\n",
        "axes[0,0].set_yticklabels(ylabels), axes[0,0].set_ylabel('Resale Price SGD ($)', size=10)\n",
        "axes[0,0].set_xlabel('Number of Parks', size=10), axes[0,1].set_xlabel('Number of Malls', size=10)\n",
        "axes[0,2].set_xlabel('Number of MRTs', size=10)\n",
        "plt.suptitle('Resale Price (Median of Each Town) VS Number of Amenities in 2km Radius (Median of Each Town)')\n",
        "plt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPo8zWuCTj_d"
      },
      "source": [
        "We can see that the number of parks, malls and MRTs are also positively related to the price of the HDB flat. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlHJX7SyTj_d"
      },
      "source": [
        "### 2.3.4 feature dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bfwh815Tj_d"
      },
      "outputs": [],
      "source": [
        "data_wide_c3.dtypes.apply(lambda x:x.kind).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBIVrsfhTj_d"
      },
      "outputs": [],
      "source": [
        "# select x variables\n",
        "model_columns = list(data_wide_c3.columns[data_wide_c3.dtypes.apply(lambda x: x.kind).isin(['f', 'u', 'i'])])\n",
        "model_columns.remove('real_price')\n",
        "model_columns.remove('resale_price')\n",
        "model_columns.remove('cpi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7dpOidgTj_d"
      },
      "outputs": [],
      "source": [
        "len(model_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ8gA4e2Tj_d"
      },
      "outputs": [],
      "source": [
        "y_label = 'real_price'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdBCqiV2Tj_d"
      },
      "outputs": [],
      "source": [
        "list(data_wide_c3.columns[data_wide_c3.dtypes.apply(lambda x: x.kind).isin(['f', 'i'])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q--ue14Tj_d"
      },
      "outputs": [],
      "source": [
        "data_wide_c3.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghVXhwzGTj_d"
      },
      "source": [
        "Check for multicolinearity to check if the indpt variables are highly correlated. We are interested in the importance of features by looking at the coefficients of the linear regression model hence we have to correct for it. VIF greater than 10 is cause for concern. Average VIF is greater than 1 then regression may be biased. Tolerance below 0.1 is a problem. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wRjW-gVTj_d"
      },
      "outputs": [],
      "source": [
        "#Correlation heatmap\n",
        "fig= plt.figure(figsize=(10,10))\n",
        "ax= sns.heatmap(data_wide_c3.select_dtypes(include=['int64','float64']).corr(), annot=True, fmt='.2g', vmin=-1, vmax=1, center=0, cmap='coolwarm_r', linecolor='black', linewidths=1, annot_kws={\"size\":7})\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Correlation Heatmap')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg3esKhYTj_d"
      },
      "source": [
        "From the above heatmap, we can see that unsurprisingly, remaining_lease_year, year_of_transaction_from_lease, whole_years_from_lease and  are correlated. Since our interest is to find out the relation with resale price, we will choose the remaining_lease_year since its correlation coefficient to the real_price is the highest as compared to the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz4Caj4NTj_e"
      },
      "outputs": [],
      "source": [
        "#create new dataset which reduces the lease year to just lease_commence_date. The cpi and the latitude and longitude are also not needed at this stage. We also want to focus on the real price instead of the resale price and we will drop the resale price as well. \n",
        "data_wide_d = data_wide_c3.copy()\n",
        "data_wide_d.drop(['year_of_transaction_from_lease', 'whole_years_from_lease', 'cpi', 'LATITUDE', 'LONGITUDE', 'resale_price'], axis=1, inplace=True)\n",
        "data_wide_d.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ckSZKfTj_e"
      },
      "source": [
        "Let us plot the heatmap again to see more clearly how the factors are correlated. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJQN-Xe3Tj_e"
      },
      "outputs": [],
      "source": [
        "#Correlation heatmap\n",
        "fig= plt.figure(figsize=(10,10))\n",
        "ax= sns.heatmap(data_wide_d.select_dtypes(include=['int64','float64']).corr(), annot=True, fmt='.2g', vmin=-1, vmax=1, center=0, cmap='coolwarm_r', linecolor='black', linewidths=1, annot_kws={\"size\":7})\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Correlation Heatmap')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h06ihfeWTj_e"
      },
      "outputs": [],
      "source": [
        "#Import library for VIF\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calc_vif(X):\n",
        "    #New drop rows with missing or infinite values\n",
        "    X= X.replace([np.inf, -np.inf],np.nan).dropna()\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    vif['tolerance'] = 1/vif.VIF\n",
        "    vif['meanVIF'] = vif.VIF.mean()\n",
        "\n",
        "    return(vif)\n",
        "\n",
        "calc_vif(data_wide_d.drop(['real_price'],axis=1).select_dtypes(include=['int64','float64']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQbgodX7Tj_e"
      },
      "source": [
        "The VIF is very high and it is not ideal. There is multi-colinearity in the data but these features are important and hence we choose to leave them in. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukuPDnTUTj_e"
      },
      "outputs": [],
      "source": [
        "#We will try to drop some more features to see if the VIF can be reduced. \n",
        "calc_vif(data_wide_d.drop(['real_price', 'num_park_2km', 'num_mall_2km', 'num_mrt_2km'],axis=1).select_dtypes(include=['int64','float64']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyNy_SqiTj_e"
      },
      "source": [
        "This is a better VIF but the floor_area and the lease_commence_date are still very high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bEAdXoxTj_e"
      },
      "outputs": [],
      "source": [
        "#We will try to drop some more features to see if the VIF can be reduced. \n",
        "calc_vif(data_wide_d.drop(['real_price', 'num_mall_2km', 'floor_area_sqm', 'remaining_lease_year'],axis=1).select_dtypes(include=['int64','float64']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61h6u1nNTj_e"
      },
      "source": [
        "If we drop floor_area_sqm and remaining_lease_year, we see that the VIF and tolerance are alot more acceptable. VIF is below 5 and tolerance is above 0.2. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6eJSFwgTj_e"
      },
      "outputs": [],
      "source": [
        "#Checking for normality\n",
        "lr_df=data_wide_d.select_dtypes(include=['int64','float64'])\n",
        "lr_df.hist(bins=50, figsize=(15,10), grid=False, edgecolor='black')\n",
        "plt.tight_layout(pad=0, rect=[0,0,0.9,0.9])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "364rQLBpTj_e"
      },
      "source": [
        "Many variables do not follow a normal distribution, and most of the distances variables have some outliers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9xfvIyRTj_e"
      },
      "outputs": [],
      "source": [
        "#We will log the real price to make it more normally distributed.\n",
        "# plot qqplot before and after log transformation\n",
        "from statsmodels.api import qqplot\n",
        "fig, ((ax1,ax2), (ax3,ax4)) = plt.subplots(2,2,figsize=(5,5))\n",
        "\n",
        "ax1.hist(lr_df['real_price'], bins=50, edgecolor='black')\n",
        "qqplot(lr_df['real_price'], line='s', ax=ax2)\n",
        "ax3.hist(np.log(lr_df['real_price']), bins=50, edgecolor='black')\n",
        "qqplot(np.log(lr_df['real_price']), line='s', ax=ax4)\n",
        "plt.suptitle('Real Price Normality Before (Top) & After (Bottom) Logging')\n",
        "plt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLHJUAM4Tj_e"
      },
      "source": [
        "Logging the real price produces a better result. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il9FQWFgTj_f"
      },
      "source": [
        "# 3 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOG0M7ajTj_f"
      },
      "source": [
        "## 3.1 Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "C7vzoFMVTj_f"
      },
      "outputs": [],
      "source": [
        "sub_model_columns = ['floor_area_sqm','remaining_lease_year','storey_range_label', 'region_Central Mature', 'region_East Mature','region_West Mature', 'region_West Non-Mature', 'region_North Non-Mature', 'region_North East Mature', 'region_North East Non-Mature','park_dist','mrt_dist','mall_dist','num_park_2km','num_mall_2km','num_mrt_2km'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F83YlBfPTj_f"
      },
      "outputs": [],
      "source": [
        "# Correlation heatmap\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax = sns.heatmap(data_wide_d[sub_model_columns].sample(10000).corr('spearman'), annot = True, fmt='.2g', \n",
        "    vmin=-1, vmax=1, center= 0, cmap= 'coolwarm_r', linecolor='black', linewidth=1, annot_kws={\"size\": 7})\n",
        "#ax.set_ylim(0 ,5)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Correlation Heatmap')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH7dJkvuTj_f"
      },
      "outputs": [],
      "source": [
        "lr_df = data_wide_d.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej0FNQs-Tj_f"
      },
      "outputs": [],
      "source": [
        "lr_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_MlsHUoTj_f"
      },
      "source": [
        "We need to scale the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix-RlEYETj_f"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit to continuous columns and transform\n",
        "scaled_columns = ['floor_area_sqm','remaining_lease_year','storey_range_label', 'region_Central Mature', 'region_East Mature','region_West Mature', 'region_West Non-Mature', 'region_North Non-Mature', 'region_North East Mature', 'region_North East Non-Mature','park_dist','mrt_dist','mall_dist','num_park_2km','num_mall_2km','num_mrt_2km'\n",
        "]\n",
        "scaler.fit(lr_df[scaled_columns])\n",
        "scaled_columns = pd.DataFrame(scaler.transform(lr_df[scaled_columns]), index=lr_df.index, columns=scaled_columns)\n",
        "\n",
        "# separate unscaled features\n",
        "unscaled_columns = lr_df.drop(scaled_columns, axis=1)\n",
        "\n",
        "# concatenate scaled and unscaled features\n",
        "lr_df = pd.concat([scaled_columns,unscaled_columns], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7AlaUKjTj_f"
      },
      "outputs": [],
      "source": [
        "lr_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNvOFOdpTj_f"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeC5hUmVTj_f"
      },
      "source": [
        "We need to scale the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmjjLEvxTj_f"
      },
      "outputs": [],
      "source": [
        "y_label='real_price'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZQjxzTyTj_f"
      },
      "outputs": [],
      "source": [
        "# split data set into train set and test set with x and y separate\n",
        "# four dataframe generated:train_y, test_x, test_y, train_x, you should arrange them in proper order\n",
        "train_x, test_x, train_y, test_y = train_test_split(lr_df[sub_model_columns], lr_df[y_label], \n",
        "    test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqZvrxkRTj_f"
      },
      "outputs": [],
      "source": [
        "# create a regressor named lin_reg with LinearRegression and fit it with train_x and np.log(train_y)\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(train_x, np.log(train_y)) #we need to log the train_y data as it is not normally distributed and we want to improve the linearity. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uETWpr6sTj_f"
      },
      "outputs": [],
      "source": [
        "lin_reg_pred = lin_reg.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVyR7CTXTj_f"
      },
      "outputs": [],
      "source": [
        "#Include Lasso and Ridge Regularization [New] \n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "#Create Lasso model [New]\n",
        "clf_L1= Lasso(alpha=0.3)\n",
        "clf_L1.fit(train_x, np.log(train_y))\n",
        "\n",
        "y_pred_L1= clf_L1.predict(test_x)\n",
        "\n",
        "print(f'Coefficients: {clf_L1.coef_}')\n",
        "print(f'Intercept: {clf_L1.intercept_}')\n",
        "print(f'R^2 score for train set: {clf_L1.score(train_x, np.log(train_y))}')\n",
        "print(f'R^2 score for test set: {clf_L1.score(test_x, np.log(test_y))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRYcsn2fTj_g"
      },
      "outputs": [],
      "source": [
        "#Create Ridge model [New]\n",
        "clf_L2= Ridge(alpha=0.3)\n",
        "clf_L2.fit(train_x, np.log(train_y))\n",
        "\n",
        "y_pred_L2= clf_L2.predict(test_x)\n",
        "\n",
        "print(f'Coefficients: {clf_L2.coef_}')\n",
        "print(f'Intercept: {clf_L2.intercept_}')\n",
        "print(f'R^2 score for train set: {clf_L2.score(train_x, np.log(train_y))}')\n",
        "print(f'R^2 score for test set: {clf_L2.score(test_x, np.log(test_y))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS0g6JnFTj_g"
      },
      "outputs": [],
      "source": [
        "#Linear Regression Coeff, intercept\n",
        "print(f'Coefficients: {lin_reg.coef_}')\n",
        "print(f'Intercept: {lin_reg.intercept_}')\n",
        "print(f'R^2 score for train set: {lin_reg.score(train_x, np.log(train_y))}')\n",
        "print(f'R^2 score for test set: {lin_reg.score(test_x, np.log(test_y))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zY50nlGDTj_g"
      },
      "outputs": [],
      "source": [
        "#Print MSE for all three models to compare the performance - output shows that ridge and linear regression are better performers than lasso \n",
        "Lasso_mse = mean_squared_error(np.log(test_y), y_pred_L1)\n",
        "Ridge_mse = mean_squared_error(np.log(test_y), y_pred_L2)\n",
        "LinearReg_mse = mean_squared_error(np.log(test_y), lin_reg_pred)\n",
        "\n",
        "print(f'Lasso MSE: {Lasso_mse:.2f}')\n",
        "print(f'Ridge MSE: {Ridge_mse:.2f}')\n",
        "print(f'Linear Regression MSE: {LinearReg_mse:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVkbao44Tj_g"
      },
      "source": [
        "The best output is the RSE and linear regression model as they have a low MSE and a high R sqaured score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo7rsk61Tj_g"
      },
      "source": [
        "Scatterplot for linear regression model R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3WIBKhTTj_g"
      },
      "outputs": [],
      "source": [
        "# scatterplot of y (observed) and yhat (predicted) for train set using the Ridge model. \n",
        "plt.style.use('default')\n",
        "fig = plt.figure(figsize=(5,3))\n",
        "ax = sns.scatterplot(x=np.log(train_y), y=lin_reg.predict(train_x), edgecolors='w', alpha=0.9, s=8)\n",
        "ax.set_xlabel('Observed (ln)')#, ax.set_xticklabels(['{:,.0f}'.format(x) + 'K' for x in ax.get_xticks()/1000])\n",
        "ax.set_ylabel('Predicted (ln)')#, ax.set_yticklabels(['{:,.0f}'.format(x) + 'K' for x in ax.get_yticks()/1000])\n",
        "ax.annotate('Adjusted R\\u00b2: ' + str(format(round(lin_reg.score(train_x, np.log(train_y)),4),'.2f')), xy=(0, 1), xytext=(25, -25),\n",
        "    xycoords='axes fraction', textcoords='offset points', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMZm9CvVTj_g"
      },
      "source": [
        "Scatterplot for Ridge Model R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy2wsDFpTj_g"
      },
      "outputs": [],
      "source": [
        "# scatterplot of y (observed) and yhat (predicted) for train set using the Ridge model. \n",
        "plt.style.use('default')\n",
        "fig = plt.figure(figsize=(5,3))\n",
        "ax = sns.scatterplot(x=np.log(train_y), y=clf_L2.predict(train_x), edgecolors='w', alpha=0.9, s=8)\n",
        "ax.set_xlabel('Observed (ln)')#, ax.set_xticklabels(['{:,.0f}'.format(x) + 'K' for x in ax.get_xticks()/1000])\n",
        "ax.set_ylabel('Predicted (ln)')#, ax.set_yticklabels(['{:,.0f}'.format(x) + 'K' for x in ax.get_yticks()/1000])\n",
        "ax.annotate('Adjusted R\\u00b2: ' + str(format(round(clf_L2.score(train_x, np.log(train_y)),4),'.2f')), xy=(0, 1), xytext=(25, -25),\n",
        "    xycoords='axes fraction', textcoords='offset points', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLAUdY2ATj_g"
      },
      "source": [
        "Summary stats for test set, OLS regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TlKKbtMTj_g"
      },
      "outputs": [],
      "source": [
        "# statsmodel method, which gives more info\n",
        "import statsmodels.api as sm\n",
        "# alternate way using statistical formula, which does not require dummy coding manually\n",
        "# https://stackoverflow.com/questions/50733014/linear-regression-with-dummy-categorical-variables\n",
        "# https://stackoverflow.com/questions/34007308/linear-regression-analysis-with-string-categorical-features-variables\n",
        "\n",
        "X_constant = sm.add_constant(test_x)\n",
        "lin_reg = sm.OLS(np.log(test_y),X_constant).fit()\n",
        "lin_reg.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wqK0QoMTj_g"
      },
      "source": [
        "Summary stats for train set, OLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGGv4NZvTj_g"
      },
      "outputs": [],
      "source": [
        "X_constant2 = sm.add_constant(train_x)\n",
        "lin_reg2 = sm.OLS(np.log(train_y),X_constant2).fit()\n",
        "lin_reg2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRbNUXpmTj_h"
      },
      "source": [
        "Check for homoscedasticity and normality of residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1Xv3dm9Tj_h"
      },
      "outputs": [],
      "source": [
        "# Homoscedasticity and Normality of Residuals\n",
        "pred = lin_reg.predict()\n",
        "resids = lin_reg.resid\n",
        "resids_studentized = lin_reg.get_influence().resid_studentized_internal\n",
        "\n",
        "fig = plt.figure(figsize=(10,3))\n",
        "\n",
        "ax1 = plt.subplot(121)\n",
        "sns.scatterplot(x=pred, y=resids_studentized, edgecolors='w', alpha=0.9, s=8)\n",
        "ax1.set_xlabel('Predicted Values')\n",
        "ax1.set_ylabel('Studentized Residuals')\n",
        "\n",
        "ax2 = plt.subplot(122)\n",
        "sns.distplot(resids_studentized, norm_hist=True, hist_kws=dict(edgecolor='w'))\n",
        "ax2.set_xlabel('Studentized Residual')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmZcLY_sTj_h"
      },
      "source": [
        "Homoscedaticity appears to be satisfied. The residuals are normally distributed around 0, satisfying the linearity and normality assumptions of the linear model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUdtJkPaTj_h"
      },
      "outputs": [],
      "source": [
        "lr_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op5iRTEgTj_h"
      },
      "source": [
        "## 3.2 random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VroB8FLCTj_h"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.datasets import make_regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZbSChsHTj_h"
      },
      "outputs": [],
      "source": [
        "sub_model_columns = ['floor_area_sqm','remaining_lease_year','region_Central Mature', 'region_East Mature','region_West Mature', 'region_West Non-Mature', 'region_North Non-Mature', 'region_North East Mature', 'region_North East Non-Mature','park_dist','mrt_dist','mall_dist','num_park_2km','num_mall_2km','num_mrt_2km', 'flat_model_apartment', 'flat_model_maisonette',\n",
        "       'flat_model_model a', 'flat_model_new generation', 'flat_model_special',\n",
        "       'flat_model_standard', 'flat_model_standard small', 'storey_range_label'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV4gZxpITj_h"
      },
      "outputs": [],
      "source": [
        "train_x2, test_x2, train_y2, test_y2 = train_test_split(lr_df[sub_model_columns], lr_df[y_label], test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UPC-cB2Tj_h"
      },
      "source": [
        "### 3.2.1 filter features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDaM4ZU4Tj_h"
      },
      "outputs": [],
      "source": [
        "# If you have too much data, you will run out of memory, \n",
        "# so start with a small amount of data to filter the features. \n",
        "train_x2_for_filter = train_x2.sample(10000, random_state=0)\n",
        "train_y2_for_filter = train_y2.loc[train_x2_for_filter.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u3RZgaITj_h"
      },
      "outputs": [],
      "source": [
        "# Validation using out-of-bag method\n",
        "# create a RandomForestRegressor named rf with n_estimators=100, oob_score=True, random_state=0\n",
        "rf_for_filter = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=0)\n",
        "rf_for_filter.fit(train_x2_for_filter, train_y2_for_filter)\n",
        "\n",
        "print(f'Out-of-bag R\\u00b2 score estimate: {rf_for_filter.oob_score_:>5.3}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jfBFbnHTj_h"
      },
      "outputs": [],
      "source": [
        "feature_importance_frame = pd.DataFrame(zip(sub_model_columns, rf_for_filter.feature_importances_), columns=['feature', 'importance'])\n",
        "feature_importance_frame.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ_-calmTj_i"
      },
      "outputs": [],
      "source": [
        "# select top 15 important features\n",
        "feature_importance_frame = feature_importance_frame.sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
        "selected = list(feature_importance_frame.iloc[:15]['feature'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET0wFwlFTj_i"
      },
      "outputs": [],
      "source": [
        "print(feature_importance_frame.iloc[:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm3VecjXTj_i"
      },
      "outputs": [],
      "source": [
        "print(selected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGnIuW4PTj_i"
      },
      "source": [
        "### 3.2.2 fit model using filtered features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBKn-wO_Tj_i"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=0, n_jobs=-1)\n",
        "#train the rf with train_x2 and train_y2 using only selected features\n",
        "rf.fit(train_x2[selected], train_y2)\n",
        "\n",
        "predicted_train = rf.predict(train_x2[selected])\n",
        "predicted_test = rf.predict(test_x2[selected])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CncxJ_M-Tj_i"
      },
      "outputs": [],
      "source": [
        "# get the prediction of rf on train_x2 and test_x2, remember to use only selected features\n",
        "predicted_train = rf.predict(train_x2[selected])\n",
        "predicted_test = rf.predict(test_x2[selected])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeTCa7V3Tj_i"
      },
      "outputs": [],
      "source": [
        "train_score = r2_score(train_y2, predicted_train)\n",
        "spearman = spearmanr(train_y2, predicted_train)\n",
        "pearson = pearsonr(train_y2, predicted_train)\n",
        "oob_mae = mean_absolute_error(train_y2, predicted_train)\n",
        "\n",
        "print(f'Train data R\\u00b2 score: {train_score:>5.3}')\n",
        "print(f'Train data Spearman correlation: {spearman[0]:.3}')\n",
        "print(f'Train data Pearson correlation: {pearson[0]:.3}')\n",
        "print(f'Train data Mean Absolute Error: {round(oob_mae)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROk8ozT3Tj_i"
      },
      "outputs": [],
      "source": [
        "test_score = r2_score(test_y2, predicted_test)\n",
        "spearman = spearmanr(test_y2, predicted_test)\n",
        "pearson = pearsonr(test_y2, predicted_test)\n",
        "oob_mae = mean_absolute_error(test_y2, predicted_test)\n",
        "\n",
        "print(f'Test data R\\u00b2 score: {test_score:>5.3}')\n",
        "print(f'Test data Spearman correlation: {spearman[0]:.3}')\n",
        "print(f'Test data Pearson correlation: {pearson[0]:.3}')\n",
        "print(f'Test data Mean Absolute Error: {round(oob_mae)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkwgO8Q9Tj_i"
      },
      "outputs": [],
      "source": [
        "#K-fold cross validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# validation by k-fold cross validation with grid search for best hyperparameters\n",
        "# hyperparameter values shown below are the tuned final values\n",
        "param_grid = {\n",
        "    'max_features': ['auto'], # max number of features considered for splitting a node\n",
        "    'max_depth': [20], # max number of levels in each decision tree\n",
        "    'min_samples_split': [15], # min number of data points placed in a node before the node is split\n",
        "    'min_samples_leaf': [2]} # min number of data points allowed in a leaf node\n",
        "rfr =GridSearchCV(RandomForestRegressor(n_estimators = 500, n_jobs=-1, random_state=0),\n",
        "                        param_grid, cv=10, scoring='r2', return_train_score=True)\n",
        "rfr.fit(train_x2, train_y2)\n",
        "print(\"Best parameters set found on Cross Validation:\\n\\n\", rfr.best_params_)\n",
        "print(\"\\nCross Validation R\\u00b2 score:\\n\\n\", rfr.best_score_.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ0EE71ATj_i"
      },
      "outputs": [],
      "source": [
        "# predict and get evaluation metrics for test set\n",
        "\n",
        "cv_predicted_test = rfr.predict(test_x2)\n",
        "\n",
        "cv_test_score = r2_score(test_y2, cv_predicted_test)\n",
        "spearman = spearmanr(test_y2, cv_predicted_test)\n",
        "pearson = pearsonr(test_y2, cv_predicted_test)\n",
        "cv_mae = mean_absolute_error(test_y2, cv_predicted_test)\n",
        "\n",
        "print(f'Test data R\\u00b2 score: {cv_test_score:>5.3}')\n",
        "print(f'Test data Spearman correlation: {spearman[0]:.3}')\n",
        "print(f'Test data Pearson correlation: {pearson[0]:.3}')\n",
        "print(f'Test data Mean Absolute Error: {round(cv_mae)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBBQDIJoTj_i"
      },
      "outputs": [],
      "source": [
        "# Get the best estimator from the GridSearchCV\n",
        "best_estimator = rfr.best_estimator_\n",
        "\n",
        "# Get the feature importances\n",
        "feature_importances = best_estimator.feature_importances_\n",
        "\n",
        "# Create a dictionary of feature importances with their corresponding column names\n",
        "feat_imp_dict = dict(zip(train_x2.columns, feature_importances))\n",
        "\n",
        "# Sort the dictionary by feature importances in descending order\n",
        "sorted_feat_imp = sorted(feat_imp_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top 10 features with their corresponding importance values\n",
        "print(\"Top 10 features with highest importance values:\")\n",
        "for i in range(10):\n",
        "    print(f\"{i+1}. {sorted_feat_imp[i][0]}: {sorted_feat_imp[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v998VyIFTj_i"
      },
      "outputs": [],
      "source": [
        "#save the models rfr and rf\n",
        "import pickle\n",
        "\n",
        "models_dict = {'rfr': rfr, 'rf': rf}\n",
        "with open('models.pkl', 'wb') as file:\n",
        "    pickle.dump(models_dict, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enEIqIowTj_i"
      },
      "outputs": [],
      "source": [
        "pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhQDdDjwTj_j"
      },
      "outputs": [],
      "source": [
        "test_y2.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UxNAQv5Tj_j"
      },
      "source": [
        "SHAP Values to see the factors that contribute to the price of different tiers of HDB "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQMqm7fXTj_j"
      },
      "outputs": [],
      "source": [
        "#Use SHAP values. \n",
        "import shap\n",
        "shap.initjs()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#flat with low resale price\n",
        "explainer = shap.TreeExplainer(rfr.best_estimator_)\n",
        "shap_values = explainer.shap_values(test_x2.iloc[[1956]])\n",
        "fig1 =shap.force_plot(explainer.expected_value[0], shap_values[0], test_x2.iloc[[1956]], matplotlib=True, show=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFYgLHA_Tj_j"
      },
      "outputs": [],
      "source": [
        "fig1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d63CLbCXTj_j"
      },
      "outputs": [],
      "source": [
        "#flat with predicted medium resale price\n",
        "explainer = shap.TreeExplainer(rfr.best_estimator_)\n",
        "shap_values = explainer.shap_values(test_x2.iloc[[6]])\n",
        "fig2= shap.force_plot(explainer.expected_value[0], shap_values[0], test_x2.iloc[[6]], matplotlib=True, show=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvHB-OQ6Tj_j"
      },
      "outputs": [],
      "source": [
        "fig2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOCLje09Tj_j"
      },
      "outputs": [],
      "source": [
        "#flat with high resale price\n",
        "explainer = shap.TreeExplainer(rfr.best_estimator_)\n",
        "shap_values = explainer.shap_values(test_x2.iloc[[172]])\n",
        "fig3 = shap.force_plot(explainer.expected_value[0], shap_values[0], test_x2.iloc[[172]], matplotlib=True, show=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5E-AwlITj_j"
      },
      "outputs": [],
      "source": [
        "fig3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5mYfwo3Tj_j"
      },
      "source": [
        "#XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XdemvEnTj_j"
      },
      "outputs": [],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HD2yZ0MTj_j"
      },
      "outputs": [],
      "source": [
        "conda install graphviz python-graphviz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# define model (complciated model listed for reference)\n",
        "XGBmodel = XGBRegressor(n_estimators=1000, max_depth=6, learning_rate=0.1,early_stopping_rounds=50)\n",
        "\n",
        "# fit model on train\n",
        "display(XGBmodel)"
      ],
      "metadata": {
        "id": "KZyt0qbFU8IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJVBU_i0Tj_j"
      },
      "outputs": [],
      "source": [
        "XGBmodel.fit(train_x2, train_y2,eval_set=[(test_x2, test_y2),(train_x2, train_y2)], verbose=50)\n",
        "\n",
        "# make a prediction\n",
        "yhat = XGBmodel.predict(test_x2, ntree_limit=XGBmodel.best_iteration)\n",
        "Root_mean_squared_error = np.mean((test_y2-yhat)**2)**.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR6sFUlsTj_j"
      },
      "outputs": [],
      "source": [
        "# summarize prediction\n",
        "print('Predicted RMSE: %.3f' % Root_mean_squared_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk8DAhgeTj_k"
      },
      "outputs": [],
      "source": [
        "xgb_test_score = r2_score(test_y2, yhat)\n",
        "spearman = spearmanr(test_y2, yhat)\n",
        "pearson = pearsonr(test_y2, yhat)\n",
        "xgb_mae = mean_absolute_error(test_y2, yhat)\n",
        "\n",
        "print(f'Test data R\\u00b2 score: {xgb_test_score:>5.3}')\n",
        "print(f'Test data Spearman correlation: {spearman[0]:.3}')\n",
        "print(f'Test data Pearson correlation: {pearson[0]:.3}')\n",
        "print(f'Test data Mean Absolute Error: {round(xgb_mae)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pylab import rcParams\n",
        "\n",
        "# set up the parameters; we can right click in jupyter notebook and save directly as .png vector file\n",
        "rcParams['figure.figsize'] = 400,400\n",
        "\n",
        "from xgboost import plot_tree\n",
        "\n",
        "plot_tree(XGBmodel, num_trees = 7)"
      ],
      "metadata": {
        "id": "l_gOxSEuVrD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This works well too: write a helper function to export DT in high res\n",
        "# credits: https://stackoverflow.com/questions/37340474/xgb-plot-tree-font-size-python\n",
        "\n",
        "def plot_tree(xgb_model, filename, rankdir='UT'):\n",
        "    \"\"\"\n",
        "    Plot the tree in high resolution\n",
        "    :param xgb_model: xgboost trained model\n",
        "    :param filename: the pdf file where this is saved\n",
        "    :param rankdir: direction of the tree: default Top-Down (UT), accepts:'LR' for left-to-right tree\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    import xgboost as xgb\n",
        "    import os\n",
        "    gvz = xgb.to_graphviz(xgb_model, num_trees=7, rankdir=rankdir)\n",
        "    _, file_extension = os.path.splitext(filename)\n",
        "    format = file_extension.strip('.').lower()\n",
        "    data = gvz.pipe(format=format)\n",
        "    full_filename = filename\n",
        "    with open(full_filename, 'wb') as f:\n",
        "        f.write(data)"
      ],
      "metadata": {
        "id": "OAiRzs6WV86Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the function to create the export file you want\n",
        "plot_tree(XGBmodel, 'XGBoost_DT.pdf')\n",
        "plot_tree(XGBmodel, 'XGBoost_DT.png')\n",
        "plot_tree(XGBmodel, 'XGBoost_DT_LR.pdf', 'LR')\n",
        "plot_tree(XGBmodel, 'XGBoost_DT_LR.png', 'LR')"
      ],
      "metadata": {
        "id": "i9fyvnCDV_Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2htEzqiTj_k"
      },
      "outputs": [],
      "source": [
        "# Get feature importance scores\n",
        "importance_scores = XGBmodel.feature_importances_\n",
        "\n",
        "# Create a list of (feature name, importance score) tuples\n",
        "features = list(zip(train_x2.columns, importance_scores))\n",
        "\n",
        "# Sort the list in descending order by importance score\n",
        "features.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top N features\n",
        "N = 20\n",
        "print(f\"Top {N} most important features:\")\n",
        "for i in range(N):\n",
        "    print(f\"{i+1}. {features[i][0]} ({features[i][1]:.4f})\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "268.932px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}